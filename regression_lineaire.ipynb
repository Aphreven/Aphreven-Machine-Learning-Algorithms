{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Linéaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Fonction linéaire des variables explicatives pour une instance/observation*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_i = \\beta_{0} + \\sum \\limits _{j=1} ^{p} x_{i,j}\\beta_{j} + \\epsilon_{i} $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p$ :  nombre de variables/colonnes \n",
    "- $x_{i,j}$ : $j^{ème}$ variable de l'instance $x_i$\n",
    "- $\\beta_j$ : $j^{ème}$ paramètre du model\n",
    "- $\\epsilon_{i}$ terme d'erreur aléatoire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Notation vectorielle*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_i = h_{\\boldsymbol{\\beta}}(\\textbf{x}_i) + \\epsilon_{i} = \\boldsymbol{\\beta} . \\textbf{x}_i + \\epsilon_{i}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\boldsymbol{\\beta}$ : vecteur des paramètres/coefficients $\\boldsymbol{\\beta} = (\\beta_0 , ... , \\beta_p)$\n",
    "- $\\textbf{x}_i$ : vecteur des variables de l'instance $\\textbf{x}_i = (1, x_{i,1} , ..., x_{i,p})$ \n",
    "- $\\boldsymbol{\\beta} . \\textbf{x} $ : produit scalaire des 2 vecteurs\n",
    "- $h_{\\boldsymbol{\\beta}}$ :  fonction hypothèse "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Notation matricielle*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{y}_{n×1} = \\boldsymbol{\\beta}_{(p+1)×1} . \\textbf{X}_{n×(p+1)} + \\boldsymbol{\\epsilon}_{n×1}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec : $ \\textbf{y} = \\begin{bmatrix} y_0 \\\\[0.3em] y_1 \\\\[0.3em] \\vdots \\\\[0.3em] y_p \\end{bmatrix}, \n",
    "\\textbf{X} = \\begin{bmatrix} 1 & x_{11} & \\cdots & x_{1p} \\\\[0.3em] 1 & x_{21} & \\cdots & x_{2p} \\\\[0.3em] \\vdots & \\vdots & \\ddots & \\vdots \\\\[0.3em] 1 & x_{n2} & \\cdots & x_{np} \\end{bmatrix}, \n",
    "\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\[0.3em] \\beta_1 \\\\[0.3em] \\vdots \\\\[0.3em] \\beta_p \\end{bmatrix}, \n",
    "\\boldsymbol{\\epsilon} = \\begin{bmatrix} \\epsilon_0 \\\\[0.3em] \\epsilon_1 \\\\[0.3em] \\vdots \\\\[0.3em] \\epsilon_p \\end{bmatrix}, \n",
    "n =$ le nombre d'observations/instances/lignes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Fonction de coût*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour une observation on a : $\\hat{y_i} = h_{\\boldsymbol{\\beta}}(\\textbf{x}_i)$ avec $\\hat{y_i}$ la valeur prédite \\\n",
    "De même on a $\\hat{\\textbf{y}} = h_{\\boldsymbol{\\beta}}(\\textbf{X})$ la résultante matricielle\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons mesurer la précision de $\\hat{\\textbf{y}}$ en utilisant une fonction de coût.\\\n",
    "Dans les modèles de régression, une mesure couramment utilisée est l'erreur quadratique moyenne (MSE en anglais : Mean Squared Error), donnée par la formule suivante : \\\n",
    "$MSE(\\textbf{X}, h) =  \\cfrac{1}{n}\\sum \\limits _{i=1} ^{n} (y_i - \\hat{y_i})^2 =  \\cfrac{1}{n}\\sum \\limits _{i=1} ^{n} (y_i - \\boldsymbol{\\beta}^T . \\textbf{x}_i)^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Estimation des coefficients de regression $\\boldsymbol{\\beta}$  via la méthode des moindre carrés*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voulons trouver $\\boldsymbol{\\hat{\\beta}}$ qui minimise la fonction de coût MSE. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Si $\\textbf{X}^T\\textbf{X}$ est inversible, on a le vecteur des paramètres approximés qui vaut : $\\boldsymbol{\\hat{\\beta}}= (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certaines situations comme $n<p$ ou des variables fortement corrélés entre elles font qu'une telle matrice n'est pas inversible.\\\n",
    " Dans ce cas, il est possible de calculer $\\boldsymbol{\\hat{\\beta}}= \\textbf{X}^+\\textbf{y}$ avec $\\textbf{X}^+$ la matrice pseudo inverse de Moore-Penrose.\\\n",
    " Cette approche est aussi plus rapide à calculer et est en pratique largement utilisé."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Complexité*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec $n$ le nombre d'observations et $p$ le nombre de variables, on obtient les complexités suivantes : \n",
    "- complexité temporelle d'apprentissage : $O(n*p^2 + p^3)$ Celle-ci se résumant a inverser la matrice $\\textbf{X}^T\\textbf{X}$ \n",
    "- complexité spatiale : $O(p)$\n",
    "- complexité temporelle de prédiction d'un set de test : $O(p)$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, widgets, FloatSlider, IntSlider\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Génération d'un jeu de données linéaire à 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 6.4 - 5.2 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction a pour paramètres $\\beta_0 = 6.4$ et $\\beta_1 = -5.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul des coefficients ${\\beta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.2115615 ],\n",
       "       [-4.96107648]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # On rajoute x0 = 1 pour chaque observations\n",
    "\n",
    "# Via inversion de matrice classique\n",
    "beta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) \n",
    "beta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.2115615 ],\n",
       "       [-4.96107648]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Via matrice pseudo-inverse\n",
    "beta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6) \n",
    "beta_best_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.2115615]), array([[-4.96107648]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Via sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malgré le bruit gaussien rajouté, on retrouve des valeurs de coefficients similaires a ceux attendus. \\\n",
    "Enfin pour effectuer des predictions de nouvelles observations il suffit de créer un vecteur et de faire le produit avec la matrice $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.2115615 ],\n",
       "       [1.25048502]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prédiction du point 0 et 1\n",
    "X_new = np.array([[0], [1]]) \n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # On rajoute x0 = 1 pour chaque observations\n",
    "y_predict = X_new_b.dot(beta_best)\n",
    "y_predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation interactive "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation de la forme $y = ax+ b $ avec comme paramètres interactifs : $a$, $b$,  et le bruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78453914e5f444a58a9dfb94dee88680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='a', max=2.0, min=-2.0), FloatSlider(value=0.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(a, b, noise_scale)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_data(a, b, noise_scale, num_points):\n",
    "    x = np.linspace(-10, 10, num_points)\n",
    "    y = a * x + b + np.random.normal(0, noise_scale, num_points)\n",
    "    return x, y\n",
    "\n",
    "def update_plot(a, b, noise_scale):\n",
    "    x, y = generate_data(a, b, noise_scale, num_points=100)\n",
    "    \n",
    "    # Fit the linear regression model\n",
    "    model = LinearRegression().fit(x.reshape(-1, 1), y)\n",
    "    a_hat, b_hat = model.coef_[0], model.intercept_\n",
    "    \n",
    "    plt.scatter(x, y, s=10, color='blue', alpha=0.6)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    \n",
    "    # Plot the estimated line\n",
    "    x_line = np.linspace(-10, 10, 2)\n",
    "    y_line = a_hat * x_line + b_hat\n",
    "    plt.plot(x_line, y_line, color='red', linestyle='--')\n",
    "\n",
    "    # Show the estimated parameters on the plot\n",
    "    plt.text(0, 9, f'a estimé: {a_hat:.2f}', fontsize=10, color='green')\n",
    "    plt.text(0, 8, f'b estimé: {b_hat:.2f}', fontsize=10, color='green')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "interact(update_plot, a=(-2.0, 2.0, 0.1), b=(-2.0, 2.0, 0.1), noise_scale=(0.1, 4.0, 0.2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Estimation des coefficients de regression $\\boldsymbol{\\beta}$  via la méthode de descente de gradient*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque le dataset comporte un très grand nombre de variables $p$, trouver les coefficients $\\beta$ devient couteux en temps de calcul, généralement de $O(p^2)$ à $O(p^3)$.\\\n",
    "Il peut etre intéressant d'utiliser des algorithmes d'optimisations pour palier a ce problème. \\\n",
    "\\\n",
    "La descente de gradient utilise une approche itérative pour trouver les coefficients optimaux du modèle.\\\n",
    "L'algorithme commence avec une estimation initiale des coefficients, qui sont mis à jour à chaque itération en utilisant la dérivée par rapport à chaque coefficient de la fonction de coût. \\\n",
    "Le processus est répété jusqu'à ce que la fonction de coût atteigne un minimum local.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Descente de gradient classique (Batch Gradient)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a l'équation de la dérivée partielle de MSE par rapport à $∂\\beta_j$ suivante : $\\cfrac{∂}{∂\\beta_j} MSE(\\boldsymbol{\\beta}) = \\cfrac{2}{n} \\sum \\limits _{i=1} ^{n} (\\boldsymbol{\\beta}^T \\textbf{x}_i-y_i)x_{i,j}$  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et le gradient suivant :  $∇MSE(\\boldsymbol{\\beta}) = \\begin{bmatrix} \\cfrac{∂}{∂\\beta_0} MSE(\\boldsymbol{\\beta}) \\\\[0.3em] \\cfrac{∂}{∂\\beta_1} MSE(\\boldsymbol{\\beta}) \\\\[0.3em] \\vdots \\\\[0.3em] \\cfrac{∂}{∂\\beta_p} MSE(\\boldsymbol{\\beta}) \\end{bmatrix} = \\cfrac{2}{n}\\textbf{X}^T(\\textbf{X}\\boldsymbol{\\beta} - \\textbf{y})$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au final on a le vecteur des paramètres qui vaudra pour chaque $k$ itérations : $\\boldsymbol{\\beta}^{k+1} = \\boldsymbol{\\beta^k} - \\eta∇MSE(\\boldsymbol{\\beta})   $\\\n",
    "avec $\\eta$ le taux d'apprentissage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Descente de gradient Stochastique (SGD)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque la matrice $X$ d'apprentissage comporte un nombre important d'observations, une technique permettant de réduire le temps de calcul au détriment\\\n",
    "de la qualité de convergence est la descente de gradient stochastique. On remplace dans notre apprentissage la matrice $X$ par une instance aléatoire $\\textbf{x}_r$ pour chaque itération\\\n",
    "Ce qui donne $MSE = (y_r - \\boldsymbol{\\beta}^T . \\textbf{x}_r)^2 = (y_r - \\hat{y_r})^2 $\\\n",
    "Les étapes suivantes restent les même que la descente de gradient classique avec le calcul de $∇MSE(\\boldsymbol{\\beta})$ puis de $\\boldsymbol{\\beta}^{k+1}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Mini batch gradient*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la même optique, le mini batch prends cette fois-ci un sous ensemble aléatoire $A\\subset{X}$ pour chaque itérations.\\\n",
    "Les étapes suivantes restent les même que la descente de gradient classique avec le calcul de $∇MSE(\\boldsymbol{\\beta})$ puis de $\\boldsymbol{\\beta}^{k+1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Complexité*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec $n$ le nombre d'observations et $p$ le nombre de variables, k le nombre d'itération,  on obtient les complexités suivantes :\n",
    " \n",
    " Batch Gradient\n",
    "- complexité temporelle d'apprentissage : $ O(knp) ∼O(kn^2)$ \n",
    "- complexité spatiale : $O(p)$\n",
    "- complexité temporelle de prédiction d'un set de test : $O(p)$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " SGD\n",
    "- complexité temporelle d'apprentissage : $O(kp)$ \n",
    "- complexité spatiale : $O(p)$\n",
    "- complexité temporelle de prédiction d'un set de test : $O(p)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Mini batch Gradient\n",
    "- complexité temporelle d'apprentissage : $O(kn_1p)$ avec $n_1$ la taille du mini batch\n",
    "- complexité spatiale : $O(p)$\n",
    "- complexité temporelle de prédiction d'un set de test : $O(p)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Notes*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La descente de gradient est sensible à la différence d'échelle entre les variables. Il faut s'assurer de qu'elles soient normalisées/standardiseés (ou autre méthodes), \\\n",
    "sous peine de prendre plus de temps à converger.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le taux d'apprentissage $\\eta$ est un hyperparamètre du model. Son choix optimal peut varier en fonction de $X$ et il peut etre interessant de tester différentes combinaisons dans le cas du Batch Gradient.\\\n",
    "Cela peut se faire via l'utilisation d'un **grid_search** en utilisant un grand nombre d'itérations et en stoppant lorsque $∇MSE(\\boldsymbol{\\beta}) <\\epsilon$ \\\n",
    "Pour le SGD et le mini batch, à cause du phénomène aléatoire de ces derniers, il est préconisé de commencer avec $\\eta$ élevé puis de le diminuer au fur et à mesure pour améliorer la convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descente de gradient classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reprends l'exemple précendent $\\beta_0 = 6.4$ et $\\beta_1 = -5.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.2115615 ],\n",
       "       [-4.96107648]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_gradient(X, y): \n",
    "    np.random.seed(20)\n",
    "\n",
    "    eta = 0.1  # learning rate\n",
    "    k_iterations = 1000\n",
    "    X_a = np.c_[np.ones((len(X), 1)), X] # On rajoute x0 = 1 pour chaque observations\n",
    "    n = len(X_a)\n",
    "    beta_path_bgd = []\n",
    "\n",
    "    beta_batch = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "    for iteration in range(k_iterations):\n",
    "        gradients = 2/n * X_a.T.dot(X_a.dot(beta_batch) - y)\n",
    "        beta_batch = beta_batch - eta * gradients\n",
    "        beta_path_bgd.append(beta_batch)\n",
    "\n",
    "    return beta_path_bgd\n",
    "\n",
    "batch_gradient(X,y)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descente de gradient stochastique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.15945708],\n",
       "       [-4.92909127]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sgd(X, y): \n",
    "    np.random.seed(20)\n",
    "    X_a = np.c_[np.ones((len(X), 1)), X] # On rajoute x0 = 1 pour chaque observations\n",
    "    n = len(X_a)\n",
    "    n_epochs = 50\n",
    "    t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "    beta_path_sgd = []\n",
    "\n",
    "    def learning_schedule(t):\n",
    "        return t0 / (t + t1)\n",
    "\n",
    "    beta_sgd = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(n):\n",
    "            random_index = np.random.randint(n)\n",
    "            xi = X_a[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            gradients = 2 * xi.T.dot(xi.dot(beta_sgd) - yi)\n",
    "            eta = learning_schedule(epoch * n + i)\n",
    "            beta_sgd = beta_sgd - eta * gradients\n",
    "            beta_path_sgd.append(beta_sgd)\n",
    "\n",
    "    return beta_path_sgd\n",
    "\n",
    "sgd(X,y)[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Batch Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.23793141],\n",
       "       [-4.92077187]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mini_batch_gradient(X,y):\n",
    "    np.random.seed(20)\n",
    "    X_a = np.c_[np.ones((len(X), 1)), X] # On rajoute x0 = 1 pour chaque observations\n",
    "    n = len(X_a)\n",
    "    beta_path_mgd = []\n",
    "\n",
    "    k_iterations = 50\n",
    "    minibatch_size = 20\n",
    "\n",
    "    beta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "    t0, t1 = 200, 1000\n",
    "    def learning_schedule(t):\n",
    "        return t0 / (t + t1)\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(k_iterations):\n",
    "        shuffled_indices = np.random.permutation(n)\n",
    "        X_a_shuffled = X_a[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        for i in range(0, n, minibatch_size):\n",
    "            t += 1\n",
    "            xi = X_a_shuffled[i:i+minibatch_size]\n",
    "            yi = y_shuffled[i:i+minibatch_size]\n",
    "            gradients = 2/minibatch_size * xi.T.dot(xi.dot(beta) - yi)\n",
    "            eta = learning_schedule(t)\n",
    "            beta = beta - eta * gradients\n",
    "            beta_path_mgd.append(beta)\n",
    "            \n",
    "    return beta_path_mgd\n",
    "\n",
    "mini_batch_gradient(X,y)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a9b23f6a5f49248faeabe9ba8363a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.5, description='a', max=4.0, min=-4.0), FloatSlider(value=4.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(a=2.5, b=4.0, k=200)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_data(a, b, num_points):\n",
    "    np.random.seed(20)\n",
    "    x = 2 * np.random.rand(num_points, 1,)\n",
    "    y = b + a* x + np.random.randn(num_points, 1)\n",
    "    return x, y\n",
    "\n",
    "def update_plot(a=2.5, b = 4.0, k = 200):\n",
    "    x, y = generate_data(a, b, num_points=200)\n",
    "    conv_points_gradient = batch_gradient(x,y)[:k] \n",
    "    conv_points_sgd = sgd(x,y)[:k] \n",
    "    conv_points_minibatch_gradient = mini_batch_gradient(x,y)[:k] \n",
    "    plt.figure(figsize=(15,5))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(np.array(conv_points_gradient)[:,0], np.array(conv_points_gradient)[:,1], \"r-s\", linewidth=1, label=\"Batch\")\n",
    "    plt.plot(np.array(conv_points_sgd)[:,0], np.array(conv_points_sgd)[:,1], \"b-o\", linewidth=1, label=\"Stochastic\")\n",
    "    plt.plot(np.array(conv_points_minibatch_gradient)[:,0], np.array(conv_points_minibatch_gradient)[:,1], \"g-+\", linewidth=1, label=\"Mini Batch\")\n",
    "    plt.legend(loc=\"upper left\", fontsize=12)\n",
    "    plt.grid()\n",
    "    plt.xlabel(r\"b\", fontsize=20)\n",
    "    plt.ylabel(r\"a\", fontsize=20)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.xlim(-0.5, 2.5)\n",
    "    plt.ylim(-4, 11)\n",
    "    plt.grid()\n",
    "    plt.scatter(x,y)\n",
    "    x_line = np.linspace(-10, 10, 2)\n",
    "\n",
    "    y_line = a * x_line + b\n",
    "    y_gd = conv_points_gradient[k-1][0] + conv_points_gradient[k-1][1]*x_line\n",
    "    y_sgd = conv_points_sgd[k-1][0] + conv_points_sgd[k-1][1]*x_line\n",
    "    y_mgd = conv_points_minibatch_gradient[k-1][0] + conv_points_minibatch_gradient[k-1][1]*x_line\n",
    "    plt.plot(x_line, y_line, color='red')\n",
    "    plt.plot(x_line, y_sgd, color='red', linestyle='--')\n",
    "    plt.plot(x_line, y_sgd, color='blue', linestyle='--')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "interact(update_plot, a=(-4.0, 4.0, 0.1), b=(-4.0, 4.0, 0.1), k=(1, 500, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_vs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf93f583ea89c76621c7eb2a4dc25d98434677c0d7c10d31e95de5f1fc2bb254"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
